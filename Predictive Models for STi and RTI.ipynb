{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ga2yIRnxiU1I"
      },
      "outputs": [],
      "source": [
        "# Step 1: Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "import neurolab as nl\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from keras.optimizers import RMSprop\n",
        "from rbflayer import RBFLayer, InitCentersRandom\n",
        "from kmeans_initializer import InitCentersKMeans\n",
        "import pyswarms as ps\n",
        "from pyswarms.single.global_best import GlobalBestPSO\n",
        "import csv, joblib, shap\n",
        "\n",
        "# Step 2: Import and Preprocess dataset\n",
        "df = pd.read_csv(\"Trapping Index Dataset.csv\")\n",
        "X = df.iloc[:, :8].values #Exttract the 8 input features\n",
        "y = df.iloc[:, 8].values #Extract ouput feature (NB:Use for RTI models)\n",
        "y = df.iloc[:, 9].values #Extract ouput feature (NB:Use for STI models)\n",
        "y = y.reshape(-1,1)\n",
        "# Split data into training and test sets (80% train set and 20% test set)\n",
        "X_train_actual, X_test_actual, y_train_actual, y_test_actual = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Scaling dataset to prevent numerical overflow\n",
        "scaler = MinMaxScaler(feature_range=(-1,1))\n",
        "X_train = scaler.fit_transform(X_train_actual)\n",
        "X_test = scaler.fit_transform(X_test_actual)\n",
        "y_train = scaler.fit_transform(y_train_actual)\n",
        "y_test = scaler.fit_transform(y_test_actual)\n",
        "y_train=y_train.reshape(-1)\n",
        "\n",
        "# Step 3: Build Predictive Models\n",
        "# Random search optimized Hyrbid Models\n",
        "## 1. SVR-RS Hybrid Model\n",
        "### Define the model\n",
        "svr_model = SVR(kernel = 'rbf')\n",
        "### parameter search range\n",
        "c_range = np.logspace(1, 3, 100)\n",
        "gamma_range = np.logspace(-2, 1, 20)\n",
        "epsilon_range = np.logspace(-3, 1, 20)\n",
        "param_grid = dict(C = c_range, gamma=gamma_range,epsilon = epsilon_range )\n",
        "### Deploy randomsearchcv\n",
        "random_search = RandomizedSearchCV(svr_model, param_distributions = param_grid, n_iter = 100, cv=5,\n",
        "                                      scoring ='neg_root_mean_squared_error',\n",
        "                                      refit= False, random_state=42)\n",
        "random_search.fit(X_train,y_train)\n",
        "### Obtain Optimal Hyperparameters\n",
        "best_params = random_search.best_params_\n",
        "best_C= best_params['C']\n",
        "best_gamma= best_params['gamma']\n",
        "best_epsilon= best_params['epsilon']\n",
        "print('C:', best_C, 'gamma:', best_gamma, 'epsilon:', best_epsilon)\n",
        "### Train with the best paramaters\n",
        "RS_svr_model = SVR(kernel = 'rbf', C=best_C, gamma=best_gamma, epsilon = best_epsilon)\n",
        "RS_svr_model.fit(X_train, y_train)\n",
        "\n",
        "## 2. RF-RS Hybrid Model\n",
        "### Define model\n",
        "rf_model = RandomForestRegressor()\n",
        "### Create the param grid for RF\n",
        "param_grid = {'n_estimators' : np.arange(100,1000,10),\n",
        "              'max_depth': np.arange(2,30),\n",
        "              'min_samples_split' : np.arange(2,25),\n",
        "              'min_samples_leaf' : np.arange(1,25)\n",
        "}\n",
        "### Deploy randomsearchcv\n",
        "random_search = RandomizedSearchCV(rf_model, param_distributions = param_grid, n_iter = 100, cv=5,\n",
        "                                      scoring ='neg_root_mean_squared_error',\n",
        "                                      refit=False, n_jobs=-1, random_state=101)\n",
        "random_search.fit(X_train,y_train)\n",
        "### Obtain Optimal Hyperparameters\n",
        "best_params = random_search.best_params_\n",
        "best_n_estimators= int(best_params['n_estimators'])\n",
        "best_max_depth = int(best_params['max_depth'])\n",
        "best_min_samples_split = int(best_params['min_samples_split'])\n",
        "best_min_samples_leaf = int(best_params['min_samples_leaf'])\n",
        "print('n_estimators:', best_n_estimators, 'max_depth:', best_max_depth, 'min_samples_split:',\n",
        "      best_min_samples_split,'min_samples_leaf:',best_min_samples_leaf)\n",
        "### Train with the best paramaters\n",
        "RS_rf_model = RandomForestRegressor(n_estimators=best_n_estimators, max_depth=best_max_depth,\n",
        "                                    min_samples_split=best_min_samples_split,\n",
        "                                    min_samples_leaf=best_min_samples_leaf)\n",
        "RS_rf_model.fit(X_train, y_train)\n",
        "\n",
        "## 3. XGB-RS Hybrid Model\n",
        "## Define model\n",
        "xgb_model = XGBRegressor()\n",
        "# Create the param grid for XGB\n",
        "param_grid = {'n_estimators' : np.arange(100,1000,10),\n",
        "              'max_depth': np.arange(3,30),\n",
        "              'gamma': np.logspace(-2, 1, 20),\n",
        "              'learning_rate': np.logspace(-2, 1, 20)\n",
        "}\n",
        "## Deploy randomsearchcv\n",
        "random_search = RandomizedSearchCV(xgb_model, param_distributions = param_grid, n_iter = 100, cv=5,\n",
        "                                      scoring ='neg_root_mean_squared_error',\n",
        "                                      refit=False, random_state=2024)\n",
        "random_search.fit(X_train,y_train)\n",
        "### Obtain Optimal Hyperparameters\n",
        "best_params = random_search.best_params_\n",
        "best_n_estimators= best_params['n_estimators']\n",
        "best_max_depth = best_params['max_depth']\n",
        "best_gamma = best_params['gamma']\n",
        "best_learning_rate = best_params['learning_rate']\n",
        "print('n_estimators:', best_n_estimators, 'max_depth:', best_max_depth, 'gamma:',\n",
        "      best_gamma,'learning_rate:',best_learning_rate)\n",
        "## Train with the best paramaters\n",
        "RS_xgb_model = XGBRegressor(n_estimators=best_n_estimators, max_depth=best_max_depth,\n",
        "                                    gamma=best_gamma,\n",
        "                                    learning_rate=best_learning_rate)\n",
        "RS_xgb_model.fit(X_train, y_train)\n",
        "\n",
        "#Particle Swarm Optimized Hybrid Models\n",
        "## 1. SVR-PSO\n",
        "### Define objective function for pso\n",
        "def svr_objective_function(params):\n",
        "    C = params[0,0]\n",
        "    gamma = params[0,1]\n",
        "    epsilon = params[0,2]\n",
        "    svr_model = SVR(kernel='rbf', C=C, gamma=gamma,epsilon=epsilon)\n",
        "    scores = cross_val_score(svr_model, X_train, y_train, cv=5,scoring='neg_mean_squared_error')\n",
        "    scores = -scores\n",
        "    mse = np.mean(scores)\n",
        "    return mse\n",
        "### Initialize PSO parameters\n",
        "lb= [100,0.01,0.001]\n",
        "ub =[1000,1,1]\n",
        "bounds = (lb,ub)\n",
        "options = {'c1': 2.05, 'c2': 2.05, 'w': 0.9}\n",
        "max_iterations = 200\n",
        "### Create optimizer\n",
        "optimizer = GlobalBestPSO(n_particles=50, dimensions=3, options=options, bounds=bounds)\n",
        "### Finding Optimal Hyperameters with PSO\n",
        "best_cost, best_params= optimizer.optimize(svr_objective_function, iters=max_iterations,)\n",
        "best_C = best_params[0]\n",
        "best_gamma = best_params[1]\n",
        "best_epsilon = best_params[2]\n",
        "print(\"Optimal SVR parameters: C=\", best_C, \", epsilon=\",best_epsilon, \", gamma=\", best_gamma)\n",
        "### Train model with optimal hyperparameters\n",
        "PSO_svr_model = SVR(C=best_C, epsilon= best_epsilon, kernel = 'rbf', gamma=best_gamma)\n",
        "PSO_svr_model.fit(X_train, y_train)\n",
        "\n",
        "### 2. RF-PSO\n",
        "### Define objective function for pso\n",
        "def rf_objective_function(params):\n",
        "  n_estimators = params[0,0]\n",
        "  max_depth = params[0,1]\n",
        "  min_samples_split = params[0,2]\n",
        "  min_samples_leaf = params[0,3]\n",
        "  rf_model = RandomForestRegressor(n_estimators = int(n_estimators),\n",
        "                                   max_depth = int(max_depth),min_samples_split = int(min_samples_split),\n",
        "                                   min_samples_leaf = int(min_samples_leaf), random_state = 42)\n",
        "  scores = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
        "  scores = -scores\n",
        "  mse = np.mean(scores)\n",
        "  return mse\n",
        "### Initialize PSO parameters\n",
        "lb = [100,2,2,1]\n",
        "ub = [1000,30,25,25]\n",
        "bounds = (lb,ub)\n",
        "options = {'c1':1.5, 'c2':2, 'w':0.8}\n",
        "max_iterations = 200\n",
        "### Create optimizer\n",
        "optimizer=ps.single.GlobalBestPSO(n_particles=100, dimensions=4, options=options, bounds=bounds)\n",
        "### Find Optimal Hyperameters with PSO\n",
        "best_cost, best_params = optimizer.optimize(rf_objective_function,iters=max_iterations)\n",
        "best_n_estimators= best_params[0]\n",
        "best_max_depth= best_params[1]\n",
        "best_min_samples_split= best_params[2]\n",
        "best_min_samples_leaf= best_params[3]\n",
        "print(\"Optimal RF parameters: n_estimators=\", best_n_estimators, \"max_depth=\" ,best_max_depth,\n",
        "      \"min_samples_split =\" ,best_min_samples_split, \"min_samples_leaf =\", best_min_samples_leaf)\n",
        "### Train model with optimal hyperparameters\n",
        "PSO_rf_model = RandomForestRegressor(n_estimators = int(best_n_estimators),max_depth = int(best_max_depth),\n",
        "                                     min_samples_split = int(best_min_samples_split),\n",
        "                                     min_samples_leaf = int(best_min_samples_leaf),random_state = 42)\n",
        "PSO_rf_model.fit(X_train, y_train)\n",
        "\n",
        "## 3. XGB-PSO\n",
        "### Define objective function for pso\n",
        "def xgb_objective_function(params):\n",
        "    n_estimators = params[0,0]\n",
        "    max_depth = params[0,1]\n",
        "    gamma = params[0,2]\n",
        "    learning_rate = params[0,3]\n",
        "    xgb_model = XGBRegressor(n_estimators = int(n_estimators),max_depth = int(max_depth),\n",
        "                                     gamma = gamma,\n",
        "                                     learning_rate = learning_rate, random_state = 42)\n",
        "    scores = cross_val_score(xgb_model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
        "    scores = -scores\n",
        "    mse = np.mean(scores)\n",
        "    return mse\n",
        "### Initialize PSO parameters\n",
        "lb = [100,3,0.01,0.1]\n",
        "ub = [1000,30,1,1]\n",
        "bounds = (lb,ub)\n",
        "options = {'c1':2.05, 'c2':2.05, 'w':0.9}\n",
        "max_iterations = 200\n",
        "### Create optimizer\n",
        "optimizer=ps.single.GlobalBestPSO(n_particles=150, dimensions=4, options=options, bounds=bounds)\n",
        "# Find Optimal Hyperameters with PSO\n",
        "best_cost, best_params = optimizer.optimize(xgb_objective_function,iters=max_iterations)\n",
        "best_n_estimators= best_params[0]\n",
        "best_max_depth= best_params[1]\n",
        "best_min_gamma= best_params[2]\n",
        "best_learning_rate= best_params[3]\n",
        "print(\"Optimal RF parameters: n_estimators=\", best_n_estimators, \"max_depth=\" ,best_max_depth,\n",
        "      \"min_gamma =\" ,best_min_gamma, \"learning_rate =\", best_learning_rate)\n",
        "#Train model with optimal hyperparameters\n",
        "\n",
        "PSO_xgb_model = XGBRegressor(n_estimators = int(best_n_estimators),\n",
        "                                     max_depth = int(best_max_depth),\n",
        "                                     min_gamma = best_min_gamma,\n",
        "                                     learning_rate = best_learning_rate,random_state = 42)\n",
        "PSO_xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Default RBFNN model\n",
        "## creating RBFLayer with centers found by KMeans clustering\n",
        "n_rbf_units = 10  # Adjust the number of RBF units based on your problem\n",
        "init_centers_kmeans = InitCentersKMeans(X_train)  # Instantiate InitCentersKMeans with your training data\n",
        "rbf_layer = RBFLayer(n_rbf_units,initializer=init_centers_kmeans, betas=2.0, input_shape=(8,))\n",
        "## Define the model architecture\n",
        "dft_rbfnn_model = Sequential()\n",
        "dft_rbfnn_model.add(rbf_layer)\n",
        "dft_rbfnn_model.add(Dense(1))  # Output layer\n",
        "dft_rbfnn_model.compile(loss='mean_squared_error', optimizer=RMSprop())\n",
        "## Train the model\n",
        "dft_rbfnn_model.fit(X_train, y_train, epochs=100, batch_size=50,verbose=1)\n",
        "\n",
        "# Step 4: Evaluate the model\n",
        "## 1. On trainset\n",
        "train_prediction = model_name.predict(X_train) #NB: replace model_name with actual model name( eg: RS_xgb_model or PSO_xgb_model)\n",
        "train_prediction = train_prediction.reshape(-1, 1)\n",
        "train_prediction_actual = scaler.inverse_transform(train_prediction)\n",
        "r2_train = r2_score(y_train_actual, train_prediction_actual)\n",
        "mse_train = mean_squared_error(y_train_actual,train_prediction_actual)\n",
        "rmse_train = np.sqrt(mse_train)\n",
        "mae_train = mean_squared_error(y_train_actual, train_prediction_actual)\n",
        "percentage_deviations = ((y_train_actual-train_prediction_actual)/y_train_actual)*100\n",
        "mpe_train = np.mean(percentage_deviations)\n",
        "mape_train = np.mean(np.abs(percentage_deviations))\n",
        "\n",
        "## 2. On testset\n",
        "test_prediction = model_name.predict(X_test) #NB: replace model_name with actual model name( eg: RS_xgb_model or PSO_xgb_model)\n",
        "test_prediction = test_prediction.reshape(-1, 1)\n",
        "test_prediction_actual = scaler.inverse_transform(test_prediction)\n",
        "r2_test = r2_score(y_test_actual, test_prediction_actual)\n",
        "mse_test = mean_squared_error(y_test_actual, test_prediction_actual)\n",
        "rmse_test = np.sqrt(mse_test)\n",
        "mae_test = mean_squared_error(y_test_actual, test_prediction_actual)\n",
        "percentage_deviations = ((y_test_actual-test_prediction_actual)/y_test_actual)*100\n",
        "mpe_test = np.mean(percentage_deviations)\n",
        "mape_test = np.mean(np.abs(percentage_deviations))\n",
        "\n",
        "print(\"Final r2 on test set:\", r2_test,\"Final RMSE on test set:\", rmse_test,\n",
        "      \"Final MAE on test set:\", mae_test, \"Final MPE on test set:\", mpe_test,\n",
        "      \"Final MAPE on test set:\", mape_test)\n",
        "print(\"Final r2 on train set:\", r2_train,\"Final RMSE on train set:\", rmse_train,\n",
        "      \"Final MAE on train set:\", mae_train, \"Final MPE on train set:\", mpe_train,\n",
        "      \"Final MAPE on train set:\", mape_train)\n",
        "\n",
        "\n",
        "# Step 5: Visualize the result\n",
        "test_prediction_actual=test_prediction_actual.reshape(-1,1)\n",
        "## For test set\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_test_actual, test_prediction_actual, color='blue', alpha=0.5)\n",
        "plt.plot([y_test_actual.min(), y_test_actual.max()], [y_test_actual.min(), y_test_actual.max()], color='red', linestyle='--')  # Diagonal line y=x\n",
        "plt.title('Scatter Plot of Predicted vs. Actual Values')\n",
        "plt.xlabel('Actual RTI') #NB: Use STI for STI models\n",
        "plt.ylabel('Predicted RTI') #NB: Use STI for STI models\n",
        "plt.gca().spines['top'].set_visible(True)\n",
        "plt.gca().spines['right'].set_visible(True)\n",
        "plt.gca().spines['bottom'].set_linewidth(1.5)\n",
        "plt.gca().spines['left'].set_linewidth(1.5)\n",
        "# Annotate RMSE and R2\n",
        "plt.annotate(f'RMSE: {rmse_test:.2f}', xy=(0.05, 0.95), xycoords='axes fraction', fontsize=12)\n",
        "plt.annotate(f'R2: {r2_test:.2f}', xy=(0.05, 0.90), xycoords='axes fraction', fontsize=12)\n",
        "plt.show()\n",
        "\n",
        "## For train set\n",
        "train_prediction_actual=train_prediction_actual.reshape(-1,1)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_train_actual, train_prediction_actual, color='blue', alpha=0.5)\n",
        "plt.plot([y_train_actual.min(), y_train_actual.max()], [y_train_actual.min(), y_train_actual.max()], color='red', linestyle='--')  # Diagonal line y=x\n",
        "plt.title('Scatter Plot of Predicted vs. Actual Values')\n",
        "plt.xlabel('Actual RTI') #NB: Use STI for STI models\n",
        "plt.ylabel('Predicted RTI') #NB: Use STI for STI models\n",
        "plt.gca().spines['top'].set_visible(True)\n",
        "plt.gca().spines['right'].set_visible(True)\n",
        "plt.gca().spines['bottom'].set_linewidth(1.5)\n",
        "plt.gca().spines['left'].set_linewidth(1.5)\n",
        "# Annotate RMSE and R2\n",
        "plt.annotate(f'RMSE: {rmse_train:.2f}', xy=(0.05, 0.95), xycoords='axes fraction', fontsize=12)\n",
        "plt.annotate(f'R2: {r2_train:.2f}', xy=(0.05, 0.90), xycoords='axes fraction', fontsize=12)\n",
        "plt.show()\n",
        "\n",
        "## SHAP plot for the best model\n",
        "### Initialize a Shapley explainer object with the model and data\n",
        "explainer = shap.Explainer(model_name, data_set, check_additivity=False)\n",
        "#NB: replace model_name with actual model name and data_set with either X_train or X_test\n",
        "# Compute Shapley values for all samples\n",
        "shap_values_train = explainer(X_train,check_additivity=False)\n",
        "feature_labels = ['Permeability', 'Porosity', 'Salinity', 'Sgr', 'Depth','Thickness',\n",
        "                  'Injection rate', 'Time']\n",
        "# Summary plot\n",
        "shap.summary_plot(shap_values_train, X_train, plot_type='bar', feature_names=feature_labels,\n",
        "                  show=True)\n",
        "\n",
        "# The end"
      ]
    }
  ]
}